{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "tensor([[    0, 11311,  4832,  5552,  1721,   346,     9,  4133,  3236,   112,\n",
      "          4832,  5378,   625,   181,  2582,  1721,  8176,  3236,   132,  4832,\n",
      "          2084,   261,  6782,  2269,  2927, 12834,  1721,  4268,  3236,   155,\n",
      "          4832,  5473, 26875, 42771,  6071,  1721,  5913,     2]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\n",
    "\n",
    "# prepare table + question\n",
    "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "question = \"\"\n",
    "\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding.keys())\n",
    "print(encoding['input_ids'])\n",
    "print(encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TapexTokenizer(name_or_path='microsoft/tapex-large-finetuned-tabfact', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "# Function to generate a random string of a given length\n",
    "def random_string(length=10):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "# Number of rows and columns to create a DataFrame with more than 1024 cells\n",
    "num_rows = 33  # 33 rows\n",
    "num_cols = 33  # 33 columns\n",
    "\n",
    "# Create the DataFrame with random strings\n",
    "data = [[random_string() for _ in range(num_cols)] for _ in range(num_rows)]\n",
    "# Generate headers for the DataFrame\n",
    "headers = [f\"Column_{i}\" for i in range(1, num_cols + 1)]\n",
    "\n",
    "# Create the DataFrame with random strings and headers\n",
    "df = pd.DataFrame(data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n",
    "from observatory.common_util.table_based_truncate import table_based_truncate, table2colList, table2str_using_columns\n",
    "table_str = table2str_using_columns(df)\n",
    "tokens = tokenizer.tokenize(table_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7697"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8888])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(df, return_tensors=\"pt\")\n",
    "print(encoding['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'ors']\n",
      "[7257, 994]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\n",
    "col_tokens = tokenizer.tokenize(\"Actors\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(col_tokens)\n",
    "print(col_tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  9119,  4832,  5552,  1721,   346,     9,  4133,  3236,   112,\n",
      "          4832,  5378,   625,   181,  2582,  1721,  8176,  3236,   132,  4832,\n",
      "          2084,   261,  6782,  2269,  2927, 12834,  1721,  4268,  3236,   155,\n",
      "          4832,  5473, 26875, 42771,  6071,  1721,  5913,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0, 9119, 4832, 5552, 1721,  346,    9, 4133, 3236,  112, 4832, 5378,\n",
      "          625,  181, 2582, 1721, 8176,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [\"Brad Pitt\"], \"Number of movies\": [\"87\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0, 9119, 4832, 5552, 1721,  346,    9, 4133, 3236,  112, 4832, 5378,\n",
      "          625,  181, 2582, 1721, 8176,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [\"Brad Pitt\"], \"Number of movies\": [\"87\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  9119,  4832,  5552,  3236,   112,  4832,  5378,   625,   181,\n",
      "          2582,  3236,   132,  4832,  2084,   261,  6782,  2269,  2927, 12834,\n",
      "          3236,   155,  4832,  5473, 26875, 42771,  6071,     2]])\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  9119,  4832,  5552,  3236,   112,  4832,  5378,   625,   181,\n",
      "          2582,  3236,   132,  4832,  2084,   261,  6782,  2269,  2927, 12834,\n",
      "             2]])\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 9119, 4832, 5552, 3236,  112, 4832, 5378,  625,  181, 2582,    2]])\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [\"Brad Pitt\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide an empty table, or all cells contain much tokens (e.g., >= 1024 tokens). Please carefully check the corresponding table with the query : .\n",
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2]])\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [], \"Number of movies\": []}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 9119, 4832, 5552, 3236,  112, 4832,    2]])\n"
     ]
    }
   ],
   "source": [
    "data = {\"Actors\": [\"\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 9119, 4832, 1437, 3236,  112, 4832,    2]])\n"
     ]
    }
   ],
   "source": [
    "data = {\"\": [\"\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "print(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provide nothing to query with respect to the table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 38])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([2, 38])\n",
      "torch.Size([2, 38])\n",
      "torch.Size([2, 38, 1024])\n",
      "torch.Size([2, 38, 1024])\n"
     ]
    }
   ],
   "source": [
    "def get_decoder_output_before_lm_head(model, tokenizer, table):\n",
    "    # Tokenize the table and question\n",
    "    encoding = tokenizer(table, \"\", return_tensors=\"pt\")\n",
    "    input_ids = encoding['input_ids']\n",
    "    print(input_ids.shape)\n",
    "    \n",
    "    attention_mask = encoding.get('attention_mask', None)\n",
    "    print(attention_mask.shape)\n",
    "    batch_input_ids = torch.cat([input_ids,input_ids], dim=0)\n",
    "    batch_attention_mask = torch.cat([attention_mask,attention_mask], dim=0)\n",
    "    print(batch_input_ids.shape)\n",
    "    print(batch_attention_mask.shape)\n",
    "    \n",
    "    # Directly use the model's BART structure to get the encoder's output\n",
    "    encoder_outputs = model.model.encoder(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "    print(encoder_outputs[0].shape)\n",
    "\n",
    "    # Retrieve decoder's output using encoder's outputs and attention mask\n",
    "    decoder_outputs = model.model.decoder(\n",
    "        input_ids=batch_input_ids, \n",
    "        encoder_hidden_states=encoder_outputs[0], \n",
    "        attention_mask=batch_attention_mask\n",
    "    )\n",
    "\n",
    "    # The first output of the decoder contains the last hidden states\n",
    "    return decoder_outputs[0]\n",
    "\n",
    "# Example usage:\n",
    "decoder_output = get_decoder_output_before_lm_head(model, tokenizer, table)\n",
    "print(decoder_output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
